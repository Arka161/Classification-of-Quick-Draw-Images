{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we ensemble 6 different VGG-16 models. Each model has its own strengths has weaknesses. Eg: Some models do better with class 1, and some do better in class 4 (our dataset has 6 classes). Thus, we ensemble the models to get an overall better confusion matrix that works well for all the classes, to make sure no class is underrepresented or classified badly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now import all required header files. We will need to run this code in a clean Python/Anaconda/Miniconda environment, to make sure there are no conflicts. This code works the best with a configured GPU. Our code was developed and testing with a gaming GPU, RTX 2060 with 6GB VRAM. You will need Numpy, Scipy, SKLearn, Pandas, Matplotlib, Tensroflow 2.0, Keras and basic Math/OS libraries to run this program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device-name is: /device:GPU:0\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device not found')\n",
    "else:\n",
    "    print(\"GPU device-name is:\", device_name)\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import math \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, AveragePooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Data Visualizations\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = np.load('train.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500,)\n"
     ]
    }
   ],
   "source": [
    "x = train_ds['arr_0']\n",
    "y = train_ds['arr_1']\n",
    "\n",
    "x = x.reshape(1500,28,28,1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below resizes the image to 80 x 80. The default size of (28, 28, 1) does not work for a VGG-16 input, as it is trained for higher dimensional data. Hence, we need the code below. Let us preprocess the image and add 3 channels (since the transfer learning model we use, VGG-16, requires 3 channels to be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "def change_size(image):\n",
    "    img = array_to_img(image, scale=False) #returns PIL Image\n",
    "    img = img.resize((80, 80)) #resize image\n",
    "    img = img.convert(mode='RGB') #makes 3 channels\n",
    "    arr = img_to_array(img) #convert back to array\n",
    "    return arr.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the above to our training data. Then convert the training labels to OneHot Encoding, using the to_cateogorical API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr_75 = [change_size(img) for img in x]\n",
    "del x\n",
    "train_arr_75 = np.array(train_arr_75)\n",
    "train_arr_75.shape\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to set up ImageDataGenerator for generating different models for different classes. There are some ImageDataGenerator Configurations that perform better for a particular class, and some that perform worse. \n",
    "\n",
    "Also, we needed to apply VGG-16's Preprocess Input API to get the input to the right format. \n",
    "\n",
    "samplewise_std_normalization = True -> Better Confusion Matrix for Class 4 \n",
    "zca_whitening = True -> Better Confusion Matrix for Class 1. \n",
    "\n",
    "If both the above booleans are set to False, then the overall confusion matrix is more balanced across all classes, with Class 1 and Class 4 being the weakest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr_75 = preprocess_input(train_arr_75)\n",
    "train_arr_75 = np.array(train_arr_75)\n",
    "train_arr_75 = train_arr_75.astype('float32')\n",
    "train_arr_75 /= 255\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(train_arr_75,y, test_size=0.1, random_state=1, stratify=y)\n",
    "\n",
    "#Model1\n",
    "image_gen1 = ImageDataGenerator(\n",
    "    #samplewise_center=True, don't\n",
    "    #featurewise_center=True, don't\n",
    "    #featurewise_std_normalization=True, don't\n",
    "    #samplewise_std_normalization = True,# accuracy -0.83, better confusion on class4\n",
    "    #zca_whitening = True,\n",
    "        zoom_range=0.2, # randomly zoom into images\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        #20 - 0.836\n",
    "        width_shift_range=0.3,  # randomly shift images horizontally (fraction of total width)\n",
    "        #0.1-  0.823, 0.4 - 0.73, 0.3-0.86\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        #0.3-0.77\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "train_generator = image_gen1.flow(train_images, \n",
    "                                 train_labels,\n",
    "                                batch_size=32,\n",
    "                                shuffle=True,\n",
    "                                seed=42)\n",
    "valid_generator = image_gen1.flow(test_images,\n",
    "                                 test_labels,\n",
    "                                batch_size=16,\n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define Kaiming Initializer for weights. Reference: https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def kaiming(shape, dtype=None):\n",
    "    return tf.random.normal(shape, dtype=dtype)*math.sqrt(2./shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define our VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = Sequential()\n",
    "model = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "#class1 = Dense(2**(j + 4), activation='relu')(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "model.compile(optimizer=keras.optimizers.Adamax(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 80, 80, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 40, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 16,819,014\n",
      "Trainable params: 16,819,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us Freeze the layers till Block 3 with the default ImageNet weights. VGG-16 is trained on ImageNet by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.name == \"block3_pool\":\n",
    "        break\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 42 steps, validate for 9 steps\n",
      "Epoch 1/4\n",
      "42/42 [==============================] - 9s 226ms/step - loss: 1.3991 - accuracy: 0.4461 - val_loss: 0.9441 - val_accuracy: 0.6111\n",
      "Epoch 2/4\n",
      "42/42 [==============================] - 3s 78ms/step - loss: 0.9417 - accuracy: 0.6608 - val_loss: 0.7634 - val_accuracy: 0.7917\n",
      "Epoch 3/4\n",
      "42/42 [==============================] - 3s 78ms/step - loss: 0.7503 - accuracy: 0.7307 - val_loss: 0.6241 - val_accuracy: 0.7708\n",
      "Epoch 4/4\n",
      "42/42 [==============================] - 3s 78ms/step - loss: 0.6492 - accuracy: 0.7663 - val_loss: 0.6261 - val_accuracy: 0.7986\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping()\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=4, \n",
    "          steps_per_epoch=train_generator.n//train_generator.batch_size,\n",
    "         validation_steps=valid_generator.n//valid_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The above run is just a sample model execution. In reality, we needed to run the model many times to get a result with good validation loss and validation accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the Confusion Matrix of the model above. To get an idea of how it looks like. We do something similar many times, and accordingly get our 5 different weights. Which are then ensembled. Since CNN has randomness/entropy, it often takes many different tests to get a good enough model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is displayed below:\n",
      "0.8466666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[18,  5,  0,  2,  0,  0],\n",
       "       [ 1, 22,  0,  0,  0,  2],\n",
       "       [ 0,  0, 25,  0,  0,  0],\n",
       "       [ 0,  0,  0, 24,  0,  1],\n",
       "       [ 0,  4,  0,  2, 14,  5],\n",
       "       [ 0,  1,  0,  0,  0, 24]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "Y_val_pred = model.predict(test_images)#, num_of_test_samples // batch_size+1\n",
    "Y_val_pred = np.argmax(Y_val_pred, axis=1)\n",
    "\n",
    "test_labels = np.argmax(test_labels, axis = 1)\n",
    "\n",
    "cf_matrix = confusion_matrix(test_labels, Y_val_pred)\n",
    "print(\"Accuracy Score is displayed below:\")\n",
    "\n",
    "print(accuracy_score(test_labels, Y_val_pred))\n",
    "categories = list(np.unique(test_labels))\n",
    "df_cm = pd.DataFrame(cf_matrix,index = categories,\n",
    "  columns = categories)\n",
    "plt.figure(figsize=(10,8))\n",
    "res = sns.heatmap(df_cm, annot=True, vmin=0.0, vmax=100.0)\n",
    "bottom, top = res.get_ylim()\n",
    "res.set_ylim(bottom + 0.5, top - 0.5)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us go over how we save a weight.\n",
    "\n",
    "\n",
    "#Uncomment the line below to save that weight\n",
    "#model.save_weights(\"80x80badClass4.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we had repeated the above steps many times till we got a good set of weights. We then saved them, and proceeded forward to do a probabilistic voting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how ensembling or soft voting is carried out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create 5 dummy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "input_tensor = Input(shape=(80, 80, 3))\n",
    "model5 = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "#model = ResNet18(input_shape=(224,224,3), weights='imagenet', classes=1000)\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model5.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model5 = Model(inputs=model5.inputs, outputs=output)\n",
    "model5.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Do not use default learning rate since it is too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "input_tensor = Input(shape=(75, 75, 3))\n",
    "model = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "#model = ResNet18(input_shape=(224,224,3), weights='imagenet', classes=1000)\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model = Model(inputs=model.inputs, outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Do not use default learning rate since it is too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "input_tensor = Input(shape=(75, 75, 3))\n",
    "model2 = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "#model = ResNet18(input_shape=(224,224,3), weights='imagenet', classes=1000)\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model2.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model2 = Model(inputs=model2.inputs, outputs=output)\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Do not use default learning rate since it is too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "input_tensor = Input(shape=(80, 80, 3))\n",
    "model6 = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "#model = ResNet18(input_shape=(224,224,3), weights='imagenet', classes=1000)\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model6.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model6 = Model(inputs=model6.inputs, outputs=output)\n",
    "model6.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#Do not use default learning rate since it is too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Sequential()\n",
    "input_tensor = Input(shape=(80, 80, 3))\n",
    "model7 = VGG16(weights = \"imagenet\",include_top=False, input_shape=(80, 80, 3))\n",
    "#model = ResNet18(input_shape=(224,224,3), weights='imagenet', classes=1000)\n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model7.layers[-1].output)\n",
    "x2 = Dropout(.2)(flat1)\n",
    "class1 = Dense(1024, activation='relu', kernel_initializer= kaiming)(x2)\n",
    "x2 = Dropout(.3)(class1)\n",
    "output = Dense(6, activation='softmax')(class1)\n",
    "model7 = Model(inputs=model7.inputs, outputs=output)\n",
    "model7.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.load_weights('badC2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.load_weights('badC490_1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7.load_weights('badC490.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('badC285ButGoodC4.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights('badC4ButGoodBalance90.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the test dataset below. This is to be used for final predictions and generating the CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = np.load('test.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us replace the last layer of model5 (the SoftMax Activation Function Layer) with an SVM. This SVM predicts on Features extracted by the VGG. This worked the best for our case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are printing the shapes of various stages of the CNN + SVM operation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 80, 80, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 40, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 16,819,014\n",
      "Trainable params: 16,819,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we had done an argmax earlier, we now need to restore the original values. This is NOT random, as we have hardcoded a random state/random seed,\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(train_arr_75,y, test_size=0.1, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 80, 80, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 80, 80, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 40, 40, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 40, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 20, 20, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 20, 20, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 10, 10, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 5, 5, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 16,819,014\n",
      "Trainable params: 16,819,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1350, 1024)\n",
      "(150, 1024)\n",
      "(1350, 1024)\n"
     ]
    }
   ],
   "source": [
    "model_featlast = Model(inputs=model5.input,outputs=model5.get_layer('dense_2').output)\n",
    "\n",
    "feat_trainlast = model_featlast.predict(train_images)\n",
    "print(feat_trainlast.shape)\n",
    "\n",
    "feat_vallast = model_featlast.predict(test_images)\n",
    "print(feat_vallast.shape)\n",
    "\n",
    "feat_trainlast = model_featlast.predict(train_images)\n",
    "print(feat_trainlast.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm3l = SVC(kernel='rbf', probability = True)\n",
    "\n",
    "svm3l.fit(feat_trainlast,np.argmax(train_labels,axis=1))\n",
    "svm3l.score(feat_trainlast,np.argmax(train_labels,axis=1))\n",
    "val_scorel = svm3l.score(feat_vallast,np.argmax(test_labels,axis=1))\n",
    "val_scorel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score is displayed below:\n",
      "0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[23,  1,  0,  1,  0,  0],\n",
       "       [ 0, 24,  0,  0,  1,  0],\n",
       "       [ 0,  0, 25,  0,  0,  0],\n",
       "       [ 0,  0,  0, 24,  0,  1],\n",
       "       [ 0,  0,  0,  2, 22,  1],\n",
       "       [ 0,  0,  0,  0,  2, 23]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = np.zeros( (test_images.shape[0],6) ) \n",
    "\n",
    "#SVM for L5\n",
    "feat_val_2 = model_featlast.predict(test_images)\n",
    "Pred_labels_1 = svm3l.predict_proba(feat_val_2)\n",
    "results = results + Pred_labels_1\n",
    "results = results + model.predict(test_images)\n",
    "results = results + model2.predict(test_images)\n",
    "results = results + model6.predict(test_images)\n",
    "results = results + model7.predict(test_images)\n",
    "#results = results + model7.predict(test_images)\n",
    "results = np.argmax(results,axis = 1)\n",
    "Y_val_pred = results\n",
    "\n",
    "\n",
    "test_labels = np.argmax(test_labels, axis = 1)\n",
    "cf_matrix = confusion_matrix(test_labels, Y_val_pred)\n",
    "print(\"Accuracy Score is displayed below:\")\n",
    "\n",
    "print(accuracy_score(test_labels, Y_val_pred))\n",
    "categories = list(np.unique(test_labels))\n",
    "df_cm = pd.DataFrame(cf_matrix,index = categories,\n",
    "  columns = categories)\n",
    "plt.figure(figsize=(10,8))\n",
    "res = sns.heatmap(df_cm, annot=True, vmin=0.0, vmax=100.0)\n",
    "bottom, top = res.get_ylim()\n",
    "res.set_ylim(bottom + 0.5, top - 0.5)\n",
    "cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above weights, we have generated our confusion matrix of 94% accuracy, and a balanced prediction across all the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = test_ds['arr_0']\n",
    "y2 = y2.reshape(60000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = np.array([change_size(img) for img in y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 80, 80, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save RAM\n",
    "y2 = np.float32(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = preprocess_input(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = np.array(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros( (y2.shape[0],6) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results + model.predict(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results + model2.predict(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results + model6.predict(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results + model7.predict(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 5 - SVM + VGG\n",
    "feat_val_1 = model_featlast.predict(y2)\n",
    "Pred_labels_1 = svm3l.predict_proba(feat_val_1)\n",
    "\n",
    "results = results + Pred_labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.argmax(results,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, ..., 5, 2, 4], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t_dataframe=pd.DataFrame(results, columns=['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataframe[\"Id\"] = t_dataframe.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"Id\",\"Category\"]\n",
    "t_dataframe=t_dataframe.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the CSV!\n",
    "\n",
    "t_dataframe.to_csv(\"answerCSV.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus, we have generated our CSV. This can be submitted to Kaggle to reproduce our accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
